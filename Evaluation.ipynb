{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5513affd-ed58-4076-a3fc-53ec2e0e91c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
    "import pytorch_lightning as pl\n",
    "from openslide import OpenSlide\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import Sampler, WeightedRandomSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ngsci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c76803-baea-41e3-9eb3-43ae21a8885a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brca_dir = Path().home() / 'datasets' / 'brca-psj-path' / \"contest-phase-2\"\n",
    "image_dir = brca_dir / \"basic-downsampling\" / \"v2-subsample-a\"\n",
    "table_dir = brca_dir / \"csv-train\"\n",
    "ndpi_dir = Path().home() / 'datasets' / 'brca-psj-path' / 'ndpi'\n",
    "clam_train_dir = brca_dir / 'clam-preprocessing-train'\n",
    "clam_test_dir = brca_dir / 'clam-preprocessing-holdout'\n",
    "\n",
    "masks_dir = clam_train_dir / 'masks'\n",
    "patches_dir = clam_train_dir / 'patches'\n",
    "patches_dir_test = clam_test_dir / 'patches'\n",
    "stitches_dir = clam_train_dir / 'stitches'\n",
    "features_h5_dir = clam_train_dir / 'resnet50-features'/ 'h5_files'\n",
    "features_pt_dir = clam_train_dir / 'resnet50-features'/ 'pt_files'\n",
    "\n",
    "logger_dir = Path().home() / \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb4d634-26c4-43d2-be42-c3f1b739c940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/default/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/venv/default/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "weights_path = \"lightning_checkpoints/lightning_logs/version_7/checkpoints/epoch=31-step=14624.ckpt\"\n",
    "backbone = resnet50(pretrained=False)\n",
    "backbone.fc = nn.Linear(backbone.fc.in_features, 5)\n",
    "\n",
    "state_dict = torch.load(weights_path, map_location=torch.device('cpu'))['state_dict']\n",
    "\n",
    "renamed_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key[6:]\n",
    "    renamed_state_dict[new_key] = value\n",
    "        \n",
    "backbone.load_state_dict(renamed_state_dict)\n",
    "backbone.eval()\n",
    "\n",
    "test_augs = T.Compose([T.ToTensor()])\n",
    "soft = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db80275e-9501-48b0-a619-ee9b53c033cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BreastBiopsy(Dataset):\n",
    "    def __init__(self, mapping_file, transform=None, target_transform=None):\n",
    "        self.dataframe = pd.read_csv(mapping_file)\n",
    "        self.target_transform = target_transform\n",
    "        self.transform = transform\n",
    "        self.demographic_column = \"race\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id = self.dataframe.loc[idx, \"slide_id\"]\n",
    "\n",
    "        with h5py.File(patches_dir / f'{slide_id}.h5', 'r') as f:\n",
    "            coords = f['coords'][:]\n",
    "            \n",
    "        random_indices = np.random.choice(coords.shape[0], size=10)\n",
    "        random_rows = coords[random_indices]\n",
    "\n",
    "        with OpenSlide(ndpi_dir / f'{slide_id}.ndpi') as slide:\n",
    "            tiles = []\n",
    "            labels = []\n",
    "            demos = []\n",
    "            \n",
    "            for row in random_rows:\n",
    "                tile_img = slide.read_region(\n",
    "                    location=row,\n",
    "                    level=0,\n",
    "                    size=(256, 256)\n",
    "                )\n",
    "\n",
    "                label = self.dataframe.loc[idx, 'stage_int']\n",
    "                demo = self.dataframe.loc[idx, self.demographic_column]\n",
    "\n",
    "                if self.transform:\n",
    "                    tile_img = self.transform(tile_img.convert('RGB'))\n",
    "                if self.target_transform:\n",
    "                    label = self.target_transform(label)\n",
    "\n",
    "                tiles.append(tile_img)\n",
    "                labels.append(torch.tensor(label))\n",
    "                demos.append(demo)\n",
    "\n",
    "        return tiles, labels, demos\n",
    "    \n",
    "class BreastBiopsyTest(Dataset):\n",
    "    def __init__(self, mapping_file, transform=None, target_transform=None):\n",
    "        self.dataframe = pd.read_csv(brca_dir / mapping_file)\n",
    "        self.target_transform = target_transform\n",
    "        self.transform = transform\n",
    "        self.demographic_column = \"race\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_id = self.dataframe.loc[idx, \"slide_id\"]\n",
    "\n",
    "        with h5py.File(patches_dir_test / f'{slide_id}.h5', 'r') as f:\n",
    "            coords = f['coords'][:]\n",
    "            \n",
    "        random_indices = np.random.choice(coords.shape[0], size=10)\n",
    "        random_rows = coords[random_indices]\n",
    "        \n",
    "        test_ndpi_dir = Path().home() / 'datasets' / 'brca-psj-path' / 'ndpi-holdout'\n",
    "        with OpenSlide(test_ndpi_dir / f'{slide_id}.ndpi') as slide:\n",
    "            tiles = []\n",
    "            labels = []\n",
    "            demos = []\n",
    "            \n",
    "            for row in random_rows:\n",
    "                tile_img = slide.read_region(\n",
    "                    location=row,\n",
    "                    level=0,\n",
    "                    size=(256, 256)\n",
    "                )\n",
    "\n",
    "                if self.transform:\n",
    "                    tile_img = self.transform(tile_img.convert('RGB'))\n",
    "\n",
    "                tiles.append(tile_img)\n",
    "\n",
    "        return tiles\n",
    "        \n",
    "#validation_data = pd.read_csv('validation.csv', nrows=2)\n",
    "validation_data = BreastBiopsy(mapping_file='validation.csv', transform=test_augs)\n",
    "validation_loader = DataLoader(validation_data, batch_size=1, num_workers=8)\n",
    "\n",
    "testing_data = BreastBiopsyTest(mapping_file='slide-manifest-holdout.csv', transform=test_augs)\n",
    "testing_loader = DataLoader(testing_data, batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84dea2e-0d59-465e-93ad-e16ce7ba8fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_test = []\n",
    "\n",
    "for batch, (tiles) in enumerate(testing_loader):\n",
    "    tiles = torch.stack(tiles).squeeze()  # Stack tiles in the batch\n",
    "    #tiles = tiles.to(device)  # Move tiles to the device (e.g., GPU)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = backbone(tiles)\n",
    "        outputs = soft(features)\n",
    "\n",
    "    predictions_test.append(outputs.cpu().numpy())\n",
    "\n",
    "# # Concatenate predictions for all tiles into a numpy array\n",
    "predictions_test = np.concatenate(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802c0d93-b2d1-493f-9f82-e52272df7ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_csv = pd.read_csv(brca_dir /'slide-manifest-holdout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b275e16-1599-483c-85ad-a695c2c0a098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_test_array = np.array(predictions_test)  # Convert the predictions list to a NumPy array\n",
    "predictions_test_array = predictions_test_array.reshape(14466, 10, 5)  # Reshape the array to (3547, 10)\n",
    "mean_predictions_test = np.mean(predictions_test_array, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e6a8c35-f49c-445c-b41b-93ff43f9a463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_csv[['prob_stage_0', 'prob_stage_1', 'prob_stage_2', 'prob_stage_3', 'prob_stage_4']] = mean_predictions_test\n",
    "grouped_test = testing_csv.groupby('biopsy_id')[['prob_stage_0', 'prob_stage_1', 'prob_stage_2', 'prob_stage_3', 'prob_stage_4']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d57c6c5a-0855-4381-b132-1dd536502a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_stage = np.argmax(np.array(grouped_test[['prob_stage_0', 'prob_stage_1', 'prob_stage_2', 'prob_stage_3', 'prob_stage_4']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abc5e435-55de-4bce-8eea-a090350539f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_test['stage_pred'] = pred_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762be076-3bff-4e70-bf20-308ebcb6b4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_test = grouped_test.reset_index()\n",
    "\n",
    "filepath = \"submission2.csv\"\n",
    "\n",
    "grouped_test.to_csv(filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b765fb4-e44d-46da-beef-1aaef29b6159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Result.SUCCESS: 1>, 'success')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngsci.stop_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd6d869-3c44-4c14-955c-28ada8385763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Result.SUCCESS: 1>, 'Success')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngsci.submit_contest_entry(\n",
    "    \"submission2.csv\", description=\"Submission 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "865a139d-b0e1-4f2f-a594-5e089305aa89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14466"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1fd29dc2-fe78-43d9-8dff-256de66be2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savetxt('data.csv', predictions_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cafecbe8-957f-451c-85ca-f5823ba97490",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fd9fd7a1990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    self._close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fd9fd7a1990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    self._close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fd9fd7a1990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
      "    reader_close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    reader_close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    self._close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    self._close()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/threading.py\", line 953, in run\n",
      "    self.run()\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'BreastBiopsy' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(predictions)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Add predictions back to the validation data pandas dataframe\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mvalidation_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BreastBiopsy' object does not support item assignment"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ngsci/.asdf/installs/python/3.10.9/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch, (tiles, labels, demos) in enumerate(validation_loader):\n",
    "    tiles = torch.stack(tiles).squeeze()  # Stack tiles in the batch\n",
    "    labels = torch.stack(labels)\n",
    "    demos = torch.stack(demos)\n",
    "    #tiles = tiles.to(device)  # Move tiles to the device (e.g., GPU)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = backbone(tiles)\n",
    "        outputs = soft(features)\n",
    "\n",
    "    predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "# # Concatenate predictions for all tiles into a numpy array\n",
    "predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50449cd3-8e28-44e6-9a03-026f7c582a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_array = np.array(predictions)  # Convert the predictions list to a NumPy array\n",
    "predictions_array = predictions_array.reshape(3547, 10, 5)  # Reshape the array to (3547, 10)\n",
    "mean_predictions = np.mean(predictions_array, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fa379b5-ae4a-408d-ae13-277ade55f668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_csv[['prediction_0', 'prediction_1', 'prediction_2', 'prediction_3', 'prediction_4']] = mean_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f44ef496-8270-4cf6-96b6-a9246306c3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped = validation_csv.groupby('biopsy_id')[['stage_int', 'prediction_0', \n",
    "                                     'prediction_1', 'prediction_2', \n",
    "                                     'prediction_3', 'prediction_4']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65c2219c-a88c-4208-863e-253068b79d22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6179475174782653"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(grouped['stage_int'], grouped[['prediction_0', 'prediction_1', 'prediction_2', 'prediction_3', 'prediction_4']], multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fdceac9-ab2e-4049-b446-fc85f02f82fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[0;32m----> 3\u001b[0m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_csv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstage_int\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43movr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:566\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_class must be in (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_roc_auc_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    570\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:639\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# validation of the input y_score\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[38;5;241m1\u001b[39m, y_score\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget scores need to be probabilities for multiclass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc, i.e. they should sum up to 1.0 over classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    642\u001b[0m     )\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# validation for multiclass parameter specifications\u001b[39;00m\n\u001b[1;32m    645\u001b[0m average_options \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(validation_csv['stage_int'], mean_predictions, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3eac85b2-0c61-4f2a-aeba-7f09141fffd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_csv = pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5b00ea7-9ccb-4e6c-a66e-f2c0e83a0bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 4, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_csv['stage_int'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72b0aadb-4d8c-4441-8c12-0ce948ba7318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3547"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bddaa1e0-35b0-4a77-8692-ab208ddd639c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.13921665, 0.27859384, 0.18984735, 0.3679208 , 0.02442137],\n",
       "        [0.11886358, 0.2591849 , 0.41398185, 0.18147205, 0.02649759],\n",
       "        [0.11014639, 0.338629  , 0.21688011, 0.3209696 , 0.01337486],\n",
       "        [0.08049978, 0.18591887, 0.32845303, 0.3612298 , 0.04389848],\n",
       "        [0.10968459, 0.2817299 , 0.32332706, 0.23210652, 0.05315192],\n",
       "        [0.10704922, 0.18278946, 0.2564581 , 0.41714498, 0.0365582 ],\n",
       "        [0.13280453, 0.26732203, 0.28716496, 0.2608255 , 0.05188296],\n",
       "        [0.09743032, 0.30762702, 0.3797851 , 0.19496673, 0.02019078],\n",
       "        [0.0756904 , 0.27659202, 0.27705577, 0.3036023 , 0.06705955],\n",
       "        [0.01527485, 0.28739876, 0.56771684, 0.09824431, 0.0313652 ]],\n",
       "       dtype=float32),\n",
       " array([[0.030879  , 0.3550534 , 0.45118526, 0.14929941, 0.01358293],\n",
       "        [0.05000063, 0.35959402, 0.43473846, 0.13185345, 0.02381345],\n",
       "        [0.0480096 , 0.60008514, 0.26854303, 0.0798969 , 0.00346532],\n",
       "        [0.05267908, 0.4256208 , 0.39088497, 0.11035256, 0.02046265],\n",
       "        [0.03088254, 0.45900628, 0.37898493, 0.11556045, 0.01556579],\n",
       "        [0.06081808, 0.31225604, 0.47535986, 0.11460041, 0.03696556],\n",
       "        [0.11422414, 0.4602174 , 0.23715176, 0.17509437, 0.01331241],\n",
       "        [0.05178179, 0.25951964, 0.44940943, 0.23346011, 0.00582903],\n",
       "        [0.0674528 , 0.3304862 , 0.4362199 , 0.13400003, 0.03184108],\n",
       "        [0.04196205, 0.3792533 , 0.43615368, 0.12318125, 0.0194498 ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d0b27a0-106c-478c-8aed-4b234afb06cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 256, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(tiles).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd5b3ad-d686-408a-8e07-9a2b3ceebc45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_single_slide(slide_id):\n",
    "    preds = []\n",
    "    with h5py.File(patches_dir / f'{slide_id}.h5', 'r') as f:\n",
    "        coords = f['coords'][:]\n",
    "\n",
    "    for x in coords:\n",
    "        with OpenSlide(ndpi_dir / f'{slide_id}.ndpi') as slide:\n",
    "            tile_img = slide.read_region(\n",
    "                location=x, \n",
    "                level=0, \n",
    "                size=(256,256)\n",
    "            )\n",
    "        \n",
    "            pred = soft(backbone(test_augs(tile_img.convert('RGB')).unsqueeze(0)))\n",
    "            preds.append(pred)\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c653059e-569f-4a4a-a2b4-c719c011ed0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m validation \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mslide_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_single_slide\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/pandas/core/series.py:4626\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4517\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4518\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4524\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4525\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4624\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mprocess_single_slide\u001b[0;34m(slide_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m OpenSlide(ndpi_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mslide_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ndpi\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m slide:\n\u001b[1;32m      8\u001b[0m         tile_img \u001b[38;5;241m=\u001b[39m slide\u001b[38;5;241m.\u001b[39mread_region(\n\u001b[1;32m      9\u001b[0m             location\u001b[38;5;241m=\u001b[39mx, \n\u001b[1;32m     10\u001b[0m             level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m     11\u001b[0m             size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     12\u001b[0m         )\n\u001b[0;32m---> 14\u001b[0m         pred \u001b[38;5;241m=\u001b[39m soft(\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_augs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtile_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m         preds\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torchvision/models/resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torchvision/models/resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/default/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation = pd.read_csv('validation.csv', nrows=2)\n",
    "\n",
    "validation['prediction'] = validation['slide_id'].apply(process_single_slide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
